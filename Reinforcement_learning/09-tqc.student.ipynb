{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b90166",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e713a4",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "In this notebook we code the Truncated Quantile Critic (TQC) algorithm using\n",
    "BBRL. This algorithm is described in [this\n",
    "paper](http://proceedings.mlr.press/v119/kuznetsov20a/kuznetsov20a.pdf).\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=U20F-MvThjM) (in the end, after SAC)\n",
    "and you can also read [the corresponding\n",
    "slides](https://dac.lip6.fr/wp-content/uploads/2022/11/12_sac.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d677378",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "\n",
    "### Installation\n",
    "\n",
    "The BBRL library is [here](https://github.com/osigaud/bbrl).\n",
    "\n",
    "Below, we import standard python packages, pytorch packages and gymnasium\n",
    "environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3cd82c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.3.5\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdcb239b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional, Iterator\n",
    "from functools import partial\n",
    "from omegaconf import OmegaConf\n",
    "from abc import abstractmethod, ABC\n",
    "from time import strftime\n",
    "from bbrl import instantiate_class\n",
    "\n",
    "\n",
    "# Useful when using a timestamp for a directory name\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daed01be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace,\n",
    "# or until a given condition is reached\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent, KWAgentWrapper\n",
    "\n",
    "# ParallelGymAgent is an agent able to execute a batch of gymnasium environments\n",
    "# with auto-resetting. These agents produce multiple variables in the workspace:\n",
    "# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/terminated’,\n",
    "# 'env/truncated', 'env/done', ’env/cumulated_reward’, ...\n",
    "#\n",
    "# When called at timestep t=0, the environments are automatically reset. At\n",
    "# timestep t>0, these agents will read the ’action’ variable in the workspace at\n",
    "# time t − 1\n",
    "from bbrl.agents.gymnasium import GymAgent, ParallelGymAgent, make_env, record_video\n",
    "\n",
    "# Replay buffers are useful to store past transitions when training\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1f2078",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Utility function for launching tensorboard\n",
    "# For Colab - otherwise, it is easier and better to launch tensorboard from\n",
    "# the terminal\n",
    "def setup_tensorboard(path):\n",
    "    path = Path(path)\n",
    "    answer = \"\"\n",
    "    if is_notebook():\n",
    "        if get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "            answer = \"y\"\n",
    "        while answer not in [\"y\", \"n\"]:\n",
    "            answer = input(\n",
    "                f\"Do you want to launch tensorboard in this notebook [y/n] \"\n",
    "            ).lower()\n",
    "\n",
    "    if answer == \"y\":\n",
    "        get_ipython().run_line_magic(\"load_ext\", \"tensorboard\")\n",
    "        get_ipython().run_line_magic(\"tensorboard\", f\"--logdir {path.absolute()}\")\n",
    "    else:\n",
    "        import sys\n",
    "        import os\n",
    "        import os.path as osp\n",
    "\n",
    "        print(\n",
    "            f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={path.absolute()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d37ea4",
   "metadata": {},
   "source": [
    "## Definition of Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a0002",
   "metadata": {},
   "source": [
    "### Functions to build networks\n",
    "\n",
    "We define a few utilitary functions to build neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d9fb5",
   "metadata": {},
   "source": [
    "The function below builds a multi-layer perceptron where the size of each\n",
    "layer is given in the `size` list. We also specify the activation function of\n",
    "neurons at each layer and optionally a different activation function for the\n",
    "final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd54dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
    "    \"\"\"Helper function to build a multi-layer perceptron (function from $\\mathbb R^n$ to $\\mathbb R^p$)\n",
    "    \n",
    "    Args:\n",
    "        sizes (List[int]): the number of neurons at each layer\n",
    "        activation (nn.Module): a PyTorch activation function (after each layer but the last)\n",
    "        output_activation (nn.Module): a PyTorch activation function (last layer)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12bab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_backbone(sizes, activation):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), activation]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a8b19",
   "metadata": {},
   "source": [
    "### Base Actor\n",
    "\n",
    "All actors should inherit from the ```BaseActor``` class, so that we can easily copy their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a8a87d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class BaseActor(Agent):\n",
    "    \"\"\" Generic class to centralize copy_parameters\"\"\"\n",
    "\n",
    "    def copy_parameters(self, other):\n",
    "        \"\"\"Copy parameters from other agent\"\"\"\n",
    "        for self_p, other_p in zip(self.parameters(), other.parameters()):\n",
    "            self_p.data.copy_(other_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a9e229",
   "metadata": {},
   "source": [
    "### Squashed Gaussian Policy\n",
    "\n",
    "Like SAC, TQC works better with a Squashed Gaussian policy, which enables the reparametrization trick.\n",
    "\n",
    "The code of the `SquashedGaussianActor` policy is below, it is the same as for SAC.\n",
    "\n",
    "It relies on a specific type of distribution, the `SquashedDiagGaussianDistribution` which is taken from [the Stable Baselines3 library](https://github.com/DLR-RM/stable-baselines3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171484b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from bbrl.utils.distributions import SquashedDiagGaussianDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba29134",
   "metadata": {},
   "source": [
    "The fact that we use the reparametrization trick is hidden inside the code of this distribution. In more details, the key is that the [`sample(self)` method](https://github.com/osigaud/bbrl/blob/master/src/bbrl/utils/distributions.py#L200) calls `rsample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c396539",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SquashedGaussianActor(BaseActor):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        backbone_dim = [state_dim] + list(hidden_layers)\n",
    "        self.layers = build_backbone(backbone_dim, activation=nn.Tanh())\n",
    "        self.backbone = nn.Sequential(*self.layers)\n",
    "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.action_dist = SquashedDiagGaussianDistribution(action_dim)\n",
    "\n",
    "    def get_distribution(self, obs: torch.Tensor):\n",
    "        backbone_output = self.backbone(obs)\n",
    "        mean = self.last_mean_layer(backbone_output)\n",
    "        std_out = self.last_std_layer(backbone_output)\n",
    "\n",
    "        std_out = std_out.clamp(-20, 2)  # as in the official code\n",
    "        std = torch.exp(std_out)\n",
    "        return self.action_dist.make_distribution(mean, std)\n",
    "\n",
    "    def forward(self, t, stochastic=False, predict_proba=False, **kwargs):\n",
    "        action_dist = self.get_distribution(self.get((\"env/env_obs\", t)))\n",
    "        if predict_proba:\n",
    "            action = self.get((\"action\", t))\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            self.set((\"logprob_predict\", t), log_prob)\n",
    "        else:\n",
    "            if stochastic:\n",
    "                action = action_dist.sample()\n",
    "            else:\n",
    "                action = action_dist.mode()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            self.set((\"action\", t), action)\n",
    "            self.set((\"action_logprobs\", t), log_prob)\n",
    "\n",
    "    def predict_action(self, obs, stochastic=False):\n",
    "        \"\"\"Predict just one action (without using the workspace)\"\"\"\n",
    "        action_dist = self.get_distribution(obs)\n",
    "        return action_dist.sample() if stochastic else action_dist.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af09d9",
   "metadata": {},
   "source": [
    "### CriticAgent\n",
    "\n",
    "As critic, TQC uses a network with several heads, defined below. As seen in the forward function, it outputs a vector of quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93592839",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TruncatedQuantileNetwork(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, n_nets, action_dim, n_quantiles):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.nets = []\n",
    "        for i in range(n_nets):\n",
    "            net = build_mlp([state_dim + action_dim] + list(hidden_layers) + [n_quantiles], activation=nn.ReLU())\n",
    "            self.add_module(f'qf{i}', net)\n",
    "            self.nets.append(net)\n",
    "\n",
    "    def forward(self, t):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.get((\"action\", t))\n",
    "        obs_act = torch.cat((obs, action), dim=1)\n",
    "        quantiles = torch.stack(tuple(net(obs_act) for net in self.nets), dim=1)\n",
    "        self.set((\"quantiles\", t), quantiles)\n",
    "        return quantiles\n",
    "\n",
    "    def predict_value(self, obs, action):\n",
    "        obs_act = torch.cat((obs, action), dim=0)\n",
    "        quantiles = torch.stack(tuple(net(obs_act) for net in self.nets), dim=1)\n",
    "        return quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a23fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Training and evaluation environments\n",
    "\n",
    "We build two environments: one for training and another one for evaluation.\n",
    "\n",
    "For training, it is more efficient to use an autoreset agent, as we do not\n",
    "want to waste time if the task is done in an environment sooner than in the\n",
    "others.\n",
    "\n",
    "By contrast, for evaluation, we just need to perform a fixed number of\n",
    "episodes (for statistics), thus it is more convenient to use a\n",
    "noautoreset agent with a set of environments and just run one episode in\n",
    "each environment. Thus we can use the `env/done` stop variable and take the\n",
    "average over the cumulated reward of all environments.\n",
    "\n",
    "See [this\n",
    "notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)\n",
    "for explanations about agents and environment agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "422dc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from bbrl.agents.gymnasium import make_env, GymAgent, ParallelGymAgent\n",
    "from functools import partial\n",
    "\n",
    "def get_env_agents(cfg, *, autoreset=True, include_last_state=True) -> Tuple[GymAgent, GymAgent]:\n",
    "    # Returns a pair of environments (train / evaluation) based on a configuration `cfg`\n",
    "    \n",
    "    # Train environment\n",
    "    train_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name, autoreset=autoreset),\n",
    "        cfg.algorithm.n_envs, \n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    # Test environment\n",
    "    eval_env_agent = ParallelGymAgent(\n",
    "        partial(make_env, cfg.gym_env.env_name), \n",
    "        cfg.algorithm.nb_evals,\n",
    "        include_last_state=include_last_state\n",
    "    ).seed(cfg.algorithm.seed)\n",
    "\n",
    "    return train_env_agent, eval_env_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6939c",
   "metadata": {},
   "source": [
    "### Building the complete training and evaluation agents\n",
    "\n",
    "In the code below we create the Squashed Gaussian actor, one critic and the corresponding target critic. Beforehand, we checked that the environment takes continuous actions (otherwise we would need a different code).\n",
    "\n",
    "An good exercise is to check that TQC also works without a target critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd1bea32",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create the TQC Agent\n",
    "def create_tqc_agent(cfg, train_env_agent, eval_env_agent):\n",
    "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
    "    assert (\n",
    "        train_env_agent.is_continuous_action()\n",
    "    ), \"TQC code dedicated to continuous actions\"\n",
    "\n",
    "    # Actor\n",
    "    actor = SquashedGaussianActor(\n",
    "        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "    )\n",
    "\n",
    "    # Train/Test agents\n",
    "    tr_agent = Agents(train_env_agent, actor)\n",
    "    ev_agent = Agents(eval_env_agent, actor)\n",
    "\n",
    "    # Builds the critics\n",
    "    critic = TruncatedQuantileNetwork(\n",
    "        obs_size, cfg.algorithm.architecture.critic_hidden_size,\n",
    "        cfg.algorithm.architecture.n_nets, act_size,\n",
    "        cfg.algorithm.architecture.n_quantiles\n",
    "    )\n",
    "    target_critic = copy.deepcopy(critic)\n",
    "\n",
    "    train_agent = TemporalAgent(tr_agent)\n",
    "    eval_agent = TemporalAgent(ev_agent)\n",
    "    #train_agent.seed(cfg.algorithm.seed)\n",
    "    return (\n",
    "        train_agent,\n",
    "        eval_agent,\n",
    "        actor,\n",
    "        critic,\n",
    "        target_critic\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db567d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### The Logger class\n",
    "\n",
    "The logger is in charge of collecting statistics during the training process.\n",
    "The logger defines the following methods, where `steps` is the number of steps\n",
    "since the training began:\n",
    "- `logger.log_losses(critic_loss: float, entropy_loss: float, actor_loss:\n",
    "  float, steps: int)`\n",
    "- `logger.log_reward_losses(self, rewards: torch.Tensor, nb_steps)`\n",
    "- `logger.add_log(log_string: float, loss: float, steps: int)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a92a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "Having logging provided under the hood is one of the features allowing you\n",
    "to save time when using RL libraries like BBRL.\n",
    "\n",
    "In these notebooks, the logger is defined as `bbrl.utils.logger.TFLogger` so as\n",
    "to use a tensorboard visualisation (see the configuration parameters).\n",
    "\n",
    "Note that the BBRL Logger is also saving the log in a readable format such\n",
    "that you can use `Logger.read_directories(...)` to read multiple logs, create\n",
    "a dataframe, and analyze many experiments afterward in a notebook for\n",
    "instance. The code for the different kinds of loggers is available in the\n",
    "[bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/src/bbrl/utils/logger.py)\n",
    "file.\n",
    "\n",
    "`instantiate_class` is an inner BBRL mechanism. The\n",
    "`instantiate_class`function is available in the\n",
    "[`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/src/bbrl/__init__.py)\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85654b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, cfg):\n",
    "        self.logger = instantiate_class(cfg.logger)\n",
    "\n",
    "    def add_log(self, log_string: float, loss: float, steps: int):\n",
    "        self.logger.add_scalar(log_string, loss.item(), steps)\n",
    "\n",
    "    # A specific function for RL algorithms having a critic, an actor and an\n",
    "    # entropy losses\n",
    "    def log_losses(\n",
    "        self, critic_loss: float, entropy_loss: float, actor_loss: float, steps: int\n",
    "    ):\n",
    "        self.add_log(\"critic_loss\", critic_loss, steps)\n",
    "        self.add_log(\"entropy_loss\", entropy_loss, steps)\n",
    "        self.add_log(\"actor_loss\", actor_loss, steps)\n",
    "\n",
    "    def log_reward_losses(self, rewards: torch.Tensor, nb_steps):\n",
    "        self.add_log(\"reward/mean\", rewards.mean(), nb_steps)\n",
    "        self.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
    "        self.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
    "        self.add_log(\"reward/median\", rewards.median(), nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8227980",
   "metadata": {},
   "source": [
    "### Setup the optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd2bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the optimizer\n",
    "def setup_optimizers(cfg, actor, critic):\n",
    "    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n",
    "    parameters = actor.parameters()\n",
    "    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n",
    "    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n",
    "    parameters = critic.parameters()\n",
    "    critic_optimizer = get_class(cfg.critic_optimizer)(\n",
    "        parameters, **critic_optimizer_args\n",
    "    )\n",
    "    return actor_optimizer, critic_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8195945",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_entropy_optimizers(cfg):\n",
    "    if cfg.algorithm.target_entropy == \"auto\":\n",
    "        entropy_coef_optimizer_args = get_arguments(cfg.entropy_coef_optimizer)\n",
    "        # Note: we optimize the log of the entropy coef which is slightly different from the paper\n",
    "        # as discussed in https://github.com/rail-berkeley/softlearning/issues/37\n",
    "        # Comment and code taken from the SB3 version of SAC\n",
    "        log_entropy_coef = torch.log(\n",
    "            torch.ones(1) * cfg.algorithm.entropy_coef\n",
    "        ).requires_grad_(True)\n",
    "        entropy_coef_optimizer = get_class(cfg.entropy_coef_optimizer)(\n",
    "            [log_entropy_coef], **entropy_coef_optimizer_args\n",
    "        )\n",
    "    else:\n",
    "        log_entropy_coef = 0\n",
    "        entropy_coef_optimizer = None\n",
    "    return entropy_coef_optimizer, log_entropy_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe132d15",
   "metadata": {},
   "source": [
    "### Compute the critic loss\n",
    "\n",
    "By contrast with the SAC version, we prepare data and compute the critic loss into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "709c813a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "        cfg, reward, must_bootstrap,\n",
    "        t_actor,\n",
    "        q_agent,\n",
    "        target_q_agent,\n",
    "        rb_workspace,\n",
    "        ent_coef\n",
    "):\n",
    "    # Compute quantiles from critic with the actions present in the buffer:\n",
    "    # at t, we have Qu  ntiles(s,a) from the (s,a) in the RB\n",
    "    q_agent(rb_workspace, t=0, n_steps=1)\n",
    "    quantiles = rb_workspace[\"quantiles\"].squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Replay the current actor on the replay buffer to get actions of the\n",
    "        # current policy\n",
    "        t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)\n",
    "        action_logprobs_next = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "        # Compute target quantiles from the target critic: at t+1, we have\n",
    "        # Quantiles(s+1,a+1) from the (s+1,a+1) where a+1 has been replaced in the RB\n",
    "\n",
    "        target_q_agent(rb_workspace, t=1, n_steps=1)\n",
    "        post_quantiles = rb_workspace[\"quantiles\"][1]\n",
    "\n",
    "        sorted_quantiles, _ = torch.sort(post_quantiles.reshape(quantiles.shape[0], -1))\n",
    "        quantiles_to_drop_total = cfg.algorithm.top_quantiles_to_drop * cfg.algorithm.architecture.n_nets\n",
    "        truncated_sorted_quantiles = sorted_quantiles[:,\n",
    "                                     :quantiles.size(-1) * quantiles.size(-2) - quantiles_to_drop_total]\n",
    "\n",
    "        # compute the target\n",
    "        logprobs = ent_coef * action_logprobs_next[1]\n",
    "        y = reward[0].unsqueeze(-1) + must_bootstrap.int().unsqueeze(-1) * cfg.algorithm.discount_factor * (\n",
    "                    truncated_sorted_quantiles - logprobs.unsqueeze(-1))\n",
    "\n",
    "    # computing the Huber loss\n",
    "    pairwise_delta = y[:, None, None, :] - quantiles[:, :, :, None]  # batch x nets x quantiles x samples\n",
    "\n",
    "    abs_pairwise_delta = torch.abs(pairwise_delta)\n",
    "    huber_loss = torch.where(abs_pairwise_delta > 1,\n",
    "                             abs_pairwise_delta - 0.5,\n",
    "                             pairwise_delta ** 2 * 0.5)\n",
    "\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "    tau = torch.arange(n_quantiles).float() / n_quantiles + 1 / 2 / n_quantiles\n",
    "    loss = (torch.abs(tau[None, None, :, None] - (pairwise_delta < 0).float()) * huber_loss).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a2bea8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Soft parameter updates\n",
    "\n",
    "To update the target critic, one uses the following equation:\n",
    "$\\theta' \\leftarrow \\tau \\theta + (1- \\tau) \\theta'$\n",
    "where $\\theta$ is the vector of parameters of the critic, and $\\theta'$ is the vector of parameters of the target critic.\n",
    "The `soft_update_params(...)` function is in charge of performing this soft update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e941dc35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def soft_update_params(net, target_net, tau):\n",
    "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27f25b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute the actor loss\n",
    "\n",
    "Again, by contrast with the SAC version, we prepare data and compute the actor loss into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a3cc60b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(ent_coef, t_actor, q_agent, rb_workspace):\n",
    "    \"\"\"Actor loss computation\n",
    "\n",
    "    :param ent_coef: The entropy coefficient $\\alpha$\n",
    "    :param t_actor: The actor agent (temporal agent)\n",
    "    :param q_agent: The critic (temporal agent) (n net of m quantiles)\n",
    "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
    "    \"\"\"\n",
    "    # Recompute the quantiles from the current policy, not from the actions in the buffer\n",
    "\n",
    "    t_actor(rb_workspace, t=0, n_steps=1, stochastic=True)\n",
    "    action_logprobs_new = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "    q_agent(rb_workspace, t=0, n_steps=1)\n",
    "    quantiles = rb_workspace[\"quantiles\"][0]\n",
    "\n",
    "    actor_loss = (ent_coef * action_logprobs_new[0] - quantiles.mean(2).mean(1))\n",
    "\n",
    "    return actor_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92530aa5",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d0e2c65",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_tqc(cfg):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg)\n",
    "    best_reward = float('-inf')\n",
    "    ent_coef = cfg.algorithm.entropy_coef\n",
    "\n",
    "    # 2) Create the environment agent\n",
    "    # train_env_agent = AutoResetGymAgent(\n",
    "    #     get_class(cfg.gym_env),\n",
    "    #     get_arguments(cfg.gym_env),\n",
    "    #     cfg.algorithm.n_envs,\n",
    "    #     cfg.algorithm.seed,\n",
    "    # )\n",
    "    # eval_env_agent = NoAutoResetGymAgent(\n",
    "    #     get_class(cfg.gym_env),\n",
    "    #     get_arguments(cfg.gym_env),\n",
    "    #     cfg.algorithm.nb_evals,\n",
    "    #     cfg.algorithm.seed,\n",
    "    # )\n",
    "    \n",
    "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
    "\n",
    "    # 3) Create the A2C Agent\n",
    "    (\n",
    "        train_agent,\n",
    "        eval_agent,\n",
    "        actor,\n",
    "        critic,\n",
    "        target_critic\n",
    "    ) = create_tqc_agent(cfg, train_env_agent, eval_env_agent)\n",
    "\n",
    "    t_actor = TemporalAgent(actor)\n",
    "    q_agent = TemporalAgent(critic)\n",
    "    target_q_agent = TemporalAgent(target_critic)\n",
    "    train_workspace = Workspace()\n",
    "\n",
    "    # Creates a replay buffer\n",
    "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg)\n",
    "    nb_steps = 0\n",
    "    tmp_steps = 0\n",
    "\n",
    "    # Initial value of the entropy coef alpha. If target_entropy is not auto,\n",
    "    # will remain fixed\n",
    "    if cfg.algorithm.target_entropy == \"auto\":\n",
    "        target_entropy = -np.prod(train_env_agent.action_space.shape).astype(np.float32)\n",
    "    else:\n",
    "        target_entropy = cfg.algorithm.target_entropy\n",
    "\n",
    "    # Training loop\n",
    "    pbar = tqdm(range(cfg.algorithm.max_epochs))\n",
    "    for epoch in pbar:\n",
    "        # Execute the agent in the workspace\n",
    "        if epoch > 0:\n",
    "            train_workspace.zero_grad()\n",
    "            train_workspace.copy_n_last_steps(1)\n",
    "            train_agent(\n",
    "                train_workspace,\n",
    "                t=1,\n",
    "                n_steps=cfg.algorithm.n_steps - 1,\n",
    "                stochastic=True,\n",
    "            )\n",
    "        else:\n",
    "            train_agent(\n",
    "                train_workspace,\n",
    "                t=0,\n",
    "                n_steps=cfg.algorithm.n_steps,\n",
    "                stochastic=True,\n",
    "            )\n",
    "\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "        action = transition_workspace[\"action\"]\n",
    "        nb_steps += action[0].shape[0]\n",
    "        rb.put(transition_workspace)\n",
    "\n",
    "        if nb_steps > cfg.algorithm.learning_starts:\n",
    "            # Get a sample from the workspace\n",
    "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "\n",
    "            done, truncated, reward, action_logprobs_rb = rb_workspace[\n",
    "                \"env/done\", \"env/truncated\", \"env/reward\", \"action_logprobs\"\n",
    "            ]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            # True if the episode reached a time limit or if the task was not done\n",
    "            # See https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md\n",
    "            must_bootstrap = ~done[1]\n",
    "\n",
    "            critic_loss = compute_critic_loss(cfg, reward, must_bootstrap,\n",
    "                                              t_actor, q_agent, target_q_agent,\n",
    "                                              rb_workspace, ent_coef)\n",
    "\n",
    "            logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
    "\n",
    "            actor_loss = compute_actor_loss(\n",
    "                ent_coef, t_actor, q_agent, rb_workspace\n",
    "            )\n",
    "            logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n",
    "\n",
    "            # Entropy coef update part ########################\n",
    "            if entropy_coef_optimizer is not None:\n",
    "                # Important: detach the variable from the graph\n",
    "                # so that we don't change it with other losses\n",
    "                # see https://github.com/rail-berkeley/softlearning/issues/60\n",
    "                ent_coef = torch.exp(log_entropy_coef.detach())\n",
    "                entropy_coef_loss = -(\n",
    "                        log_entropy_coef * (action_logprobs_rb + target_entropy)\n",
    "                ).mean()\n",
    "                entropy_coef_optimizer.zero_grad()\n",
    "                # We need to retain the graph because we reuse the\n",
    "                # action_logprobs are used to compute both the actor loss and\n",
    "                # the critic loss\n",
    "                entropy_coef_loss.backward(retain_graph=True)\n",
    "                entropy_coef_optimizer.step()\n",
    "                logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, nb_steps)\n",
    "                logger.add_log(\"entropy_coef\", ent_coef, nb_steps)\n",
    "\n",
    "            # Actor update part ###############################\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                actor.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Critic update part ###############################\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                critic.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            critic_optimizer.step()\n",
    "            ####################################################\n",
    "\n",
    "            # Soft update of target q function\n",
    "            tau = cfg.algorithm.tau_target\n",
    "            soft_update_params(critic, target_critic, tau)\n",
    "            # soft_update_params(actor, target_actor, tau)\n",
    "\n",
    "        # Evaluate ###########################################\n",
    "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
    "            tmp_steps = nb_steps\n",
    "            eval_workspace = Workspace()  # Used for evaluation\n",
    "            eval_agent(\n",
    "                eval_workspace,\n",
    "                t=0,\n",
    "                stop_variable=\"env/done\",\n",
    "                stochastic=False,\n",
    "            )\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            logger.log_reward_losses(mean, nb_steps)\n",
    "\n",
    "            pbar.set_description(f\"nb_steps: {nb_steps}, reward: {mean:.3f}\")\n",
    "            if cfg.save_best and mean > best_reward:\n",
    "                best_reward = mean\n",
    "                directory = f\"./agents/{cfg.gym_env.env_name}/tqc_agent/\"\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                filename = directory + cfg.gym_env.env_name + \"#tqc#team\" + str(mean.item()) + \".agt\"\n",
    "                actor.save_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c1d4a",
   "metadata": {},
   "source": [
    "## Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a385577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "  \"save_best\": False,\n",
    "  \"logger\":{\n",
    "    \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "    \"log_dir\": \"./tblogs/\" + str(time.time()),\n",
    "    \"cache_size\": 10000,\n",
    "    \"every_n_seconds\": 10,\n",
    "    \"verbose\": False,    \n",
    "    },\n",
    "\n",
    "  \"algorithm\":{\n",
    "    \"seed\": 1,\n",
    "    \"n_envs\": 1,\n",
    "    \"n_steps\": 512,\n",
    "    \"n_updates\": 512,\n",
    "    \"buffer_size\": 1e6,\n",
    "    \"batch_size\": 256,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"nb_evals\":10,\n",
    "    \"eval_interval\": 2000,\n",
    "    \"learning_starts\": 10000,\n",
    "    \"max_epochs\": 8000,\n",
    "    \"discount_factor\": 0.98,\n",
    "    \"entropy_coef\": 1e-7,\n",
    "    \"target_entropy\": \"auto\",\n",
    "    \"tau_target\": 0.05,\n",
    "    \"top_quantiles_to_drop\": 2,\n",
    "    \"architecture\":{\n",
    "      \"actor_hidden_size\": [32, 32],\n",
    "      \"critic_hidden_size\": [256, 256],\n",
    "      \"n_nets\": 2,\n",
    "      \"n_quantiles\": 25,\n",
    "    },\n",
    "  },\n",
    "  \"gym_env\":{\n",
    "    \"classname\": \"__main__.make_gym_env\",\n",
    "    \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "  \"actor_optimizer\":{\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "    },\n",
    "  \"critic_optimizer\":{\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "    },\n",
    "  \"entropy_coef_optimizer\":{\n",
    "    \"classname\": \"torch.optim.Adam\",\n",
    "    \"lr\": 1e-3,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbb9e3",
   "metadata": {},
   "source": [
    "### Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4676), started 1:54:02 ago. (Use '!kill 4676' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-36c87a56fc4a36f4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-36c87a56fc4a36f4\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d55781a57a4e28ae3e0d0fb61a9a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tmp\n",
    "\n",
    "config=OmegaConf.create(params)\n",
    "torch.manual_seed(config.algorithm.seed)\n",
    "run_tqc(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ec21a",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- use the same code on the Pendulum-v1 environment. This one is harder to tune. Get the parameters from the [rl-baseline3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) and see if you manage to get SAC working on Pendulum"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
